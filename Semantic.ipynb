{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "W-4v85Ezl-De",
        "3GpNNBp8pOfJ",
        "0WAzPHY96CgV",
        "ozpVZpdn6OQy",
        "4iSoClBHlhoh",
        "SO71Zdkklnph",
        "6ETxlZ_Bl5Og",
        "ez_YImKPnxmg",
        "a5wyOM7un7sv",
        "fm30zBlwm8x6",
        "Mx3c4KfTvs5Q",
        "CNT5Omjmvzek",
        "ZCKFGIhypAm3",
        "fCM_P8XXgGqA",
        "sREvEin2voei"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-4v85Ezl-De"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets huggingface_hub fsspec"
      ],
      "metadata": {
        "id": "CUBZQUpgSLip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8i5P9oPWW7rw"
      },
      "outputs": [],
      "source": [
        "#config\n",
        "vocab_size=30000\n",
        "special_tokens=[\"CLS\",\"SEP\",\"UNK\",\"PAD\"]\n",
        "n_segments=2\n",
        "max_len=350\n",
        "embedd_dim=768\n",
        "n_layers=8\n",
        "attn_heads=12\n",
        "dropout=0.1\n",
        "d_ff=1600\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GpNNBp8pOfJ"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbjBtubDi1sN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "class InputEmbedding(nn.Module):\n",
        "  def __init__(self,vocab_size:int,d_model:int)->None:\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.vocab_size=vocab_size\n",
        "    self.embedd=nn.Embedding(vocab_size,self.d_model)\n",
        "  def forward(self,x):\n",
        "  #(batch,seq_len)-->(batch,seq_len,d_model)\n",
        "    return self.embedd(x)*math.sqrt(self.d_model)\n",
        "\n",
        "class SegmentEmbedding(nn.Module):\n",
        "  def __init__(self,n_segments:int,d_model:int)->None:\n",
        "    super().__init__()\n",
        "    self.segment_embedd=nn.Embedding(n_segments,d_model)\n",
        "  def forward(self,x):\n",
        "    return self.segment_embedd(x)\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "  def __init__(self,seq_len:int,d_model:int,dropout:float)->None:\n",
        "    super().__init__()\n",
        "    self.seq_len=seq_len\n",
        "    self.d_model=d_model\n",
        "    self.drop=nn.Dropout(dropout)\n",
        "    pe=torch.zeros(seq_len,d_model)\n",
        "    position=torch.arange(0,seq_len,dtype=torch.float).unsqueeze(1)\n",
        "    div_term=torch.exp(torch.arange(0,d_model,2).float()*(-math.log(10000.0)/d_model))\n",
        "    pe[:,0::2]=torch.sin(position*div_term)\n",
        "    pe[:,1::2]=torch.cos(position*div_term)\n",
        "\n",
        "    pe=pe.unsqueeze(0)  #adding batch dim\n",
        "    self.register_buffer(\"pe\",pe)\n",
        "  def forward(self,x):\n",
        "    x=x+ self.pe[:,:x.shape[1],:].detach()\n",
        "    return self.drop(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO3BD2ju5dde"
      },
      "outputs": [],
      "source": [
        "class full_embeddings(nn.Module):\n",
        "  def __init__(self,src_emb:InputEmbedding,pe_emb:PositionalEmbedding,se_emb:SegmentEmbedding,sep_input_id):\n",
        "    super().__init__()\n",
        "    self.src_emb=src_emb\n",
        "    self.pe_emb=pe_emb\n",
        "    self.se_emb=se_emb\n",
        "    self.sep_input_id=sep_input_id\n",
        "\n",
        "\n",
        "  def forward(self,input_ids,segment_ids):\n",
        "\n",
        "    x=self.pe_emb(self.src_emb(input_ids)+self.se_emb(segment_ids))\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WAzPHY96CgV"
      },
      "source": [
        "# Normalization block,residual block,feedforward block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIre-xRfpXUI"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self,d_model, eps:float=1e-6)->None:\n",
        "    super().__init__()\n",
        "    self.eps=eps\n",
        "    self.alpha=nn.Parameter(torch.ones(d_model))\n",
        "    self.bias=nn.Parameter(torch.zeros(d_model))\n",
        "  def forward(self,x):\n",
        "    mean=x.mean(dim=-1,keepdim=True)\n",
        "    std=x.std(dim=-1,keepdim=True)\n",
        "\n",
        "    return self.alpha*(x-mean)/(std+self.eps) +self.bias\n",
        "\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "  def __init__(self,d_model:int,d_ff:int,dropout:float)->None:\n",
        "    super().__init__()\n",
        "    self.linear_1=nn.Linear(d_model,d_ff)\n",
        "    self.linear_2=nn.Linear(d_ff,d_model)\n",
        "    self.drop=nn.Dropout(dropout)\n",
        "    # self.relu=nn.ReLU()\n",
        "  def forward(self,x):\n",
        "    x=torch.relu(self.linear_1(x))\n",
        "    x=self.drop(x)\n",
        "    return self.linear_2(x)\n",
        "\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self,d_model,drop:float)->None:\n",
        "    super().__init__()\n",
        "    self.norm=LayerNormalization(d_model)\n",
        "    self.drop=nn.Dropout(drop)\n",
        "  def forward(self,x,sublayer):\n",
        "    return x+self.drop(sublayer(self.norm(x)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozpVZpdn6OQy"
      },
      "source": [
        "# Attention Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-Va0JLv6RSl"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_model:int,heads:int,dropout:float):\n",
        "    super().__init__()\n",
        "    self.heads=heads\n",
        "    assert d_model % heads==0, \"d_model not divisible by heads\"\n",
        "    self.d_k=d_model//heads\n",
        "    self.heads=heads\n",
        "    self.q=nn.Linear(d_model,d_model)\n",
        "    self.k=nn.Linear(d_model,d_model)\n",
        "    self.v=nn.Linear(d_model,d_model)\n",
        "    self.w_o=nn.Linear(d_model,d_model)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "  def attention(self,q,k,v,mask,dropout):\n",
        "    d_k=q.shape[-1]\n",
        "    attention_score=q@k.transpose(-2,-1)/math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "      mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "      attention_score=attention_score.masked_fill(mask==0,-1e9)\n",
        "    attention_score=torch.softmax(attention_score,dim=-1)\n",
        "    if dropout is not None:\n",
        "      attention_score=dropout(attention_score)\n",
        "    return attention_score@v, attention_score\n",
        "\n",
        "  def forward(self,q,k,v,mask):\n",
        "    q=self.q(q)\n",
        "    k=self.k(k)\n",
        "    v=self.v(v)\n",
        "\n",
        "    q=q.view(q.shape[0],q.shape[1],self.heads,self.d_k).transpose(1,2)\n",
        "    k=k.view(k.shape[0],k.shape[1],self.heads,self.d_k).transpose(1,2)\n",
        "    v=v.view(v.shape[0],v.shape[1],self.heads,self.d_k).transpose(1,2)\n",
        "\n",
        "    x,attention_score=self.attention(q,k,v,mask,self.dropout)\n",
        "    x=x.transpose(1,2).contiguous().view(x.shape[0],-1,self.heads*self.d_k)\n",
        "    x=self.w_o(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iSoClBHlhoh"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9McmRN9MmWmF"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self,d_model,d_ff,dropout,heads):\n",
        "    super().__init__()\n",
        "    self.feedfwd=FeedForwardNetwork(d_model,d_ff,dropout)\n",
        "\n",
        "    self.residual=nn.ModuleList([ResidualConnection(d_model,dropout),ResidualConnection(d_model,dropout)])\n",
        "    self.attention=MultiHeadAttention(d_model,heads,dropout)\n",
        "  def forward(self, x,mask):\n",
        "    x=self.residual[0](x,lambda x: self.attention(x,x,x,mask))\n",
        "    x=self.residual[1](x,self.feedfwd)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,d_model,d_ff,dropout,heads,n_layers):\n",
        "    super().__init__()\n",
        "    self.encoders=nn.ModuleList([EncoderBlock(d_model,d_ff,dropout,heads) for _ in range(n_layers)])\n",
        "  def forward(self,x,mask):\n",
        "    for layer in self.encoders:\n",
        "      x=layer(x,mask)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO71Zdkklnph"
      },
      "source": [
        "# Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRzaJKUcu1OY"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self,d_model:int,d_ff:int,dropout:float,output_size:int=3)->None:\n",
        "    super().__init__()\n",
        "    self.classifier=nn.Sequential(nn.Linear(d_model,d_ff),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.Dropout(dropout),\n",
        "                                  nn.Linear(d_ff,1024),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.Dropout(dropout),\n",
        "                                  nn.Linear(1024,output_size),\n",
        "                                  # nn.Sigmoid()\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.classifier(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ETxlZ_Bl5Og"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRMM8S-wHsdW"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,encoder:Encoder,embeddings:full_embeddings)->None:\n",
        "    super().__init__()\n",
        "    self.encoder=encoder\n",
        "    # self.classifier=classifier\n",
        "    self.emb=embeddings\n",
        "  def forward(self, x,segment_ids,mask):\n",
        "    # B,S,E=x.shape\n",
        "    x=self.emb(x,segment_ids)\n",
        "    output=self.encoder(x,mask)\n",
        "    cls=output[:,0,:]\n",
        "    cls=cls.squeeze(1)\n",
        "    # logits=self.classifier(cls)\n",
        "    return cls\n",
        "def build_transformer(vocab_size:int,n_segments:int,embedd_dim:int,max_len:int,n_layers:int,attn_heads:int,dropout:float,d_ff:int,c_d_ff:int,nli_pretrain:bool,sep_input_id:int=2)-> Transformer:\n",
        "  input_emb=InputEmbedding(vocab_size,embedd_dim)\n",
        "  seg_emb=SegmentEmbedding(n_segments,embedd_dim)\n",
        "  pe_emb=PositionalEmbedding(max_len,embedd_dim,dropout)\n",
        "\n",
        "  full_emb=full_embeddings(input_emb,pe_emb,seg_emb,sep_input_id)\n",
        "\n",
        "  encoder=Encoder(embedd_dim,d_ff,dropout,attn_heads,n_layers)\n",
        "\n",
        "  transformer=Transformer(encoder,full_emb)\n",
        "\n",
        "  for p in transformer.parameters():\n",
        "    if p.dim()>1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "  if nli_pretrain:\n",
        "    classifier=Classifier(embedd_dim,d_ff,dropout)\n",
        "    for p in classifier.parameters():\n",
        "      if p.dim()>1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "    return transformer, classifier\n",
        "\n",
        "\n",
        "  return transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez_YImKPnxmg"
      },
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abLXqKPyuMEI"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "trainer = WordPieceTrainer(vocab_size=vocab_size,special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\"])\n",
        "\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "files = [f\"data/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "files=[f\"/content/train-00000-of-00002.raw\",\"/content/train-00001-of-00002.raw\"]\n",
        "tokenizer.train(files, trainer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFIos9mAwEz3"
      },
      "outputs": [],
      "source": [
        "tokenizer.save(\"/content/tokenizer-wiki.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvUzQqtywUIE"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer.from_file(\"/content/tokenizer-wiki.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4pgViPmwVSD"
      },
      "outputs": [],
      "source": [
        "from tokenizers.processors import TemplateProcessing\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
        "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
        "\n",
        "    ],\n",
        ")\n",
        "\n",
        "tokenizer.enable_padding(pad_id=tokenizer.token_to_id(\"[PAD]\"), pad_token=\"[PAD]\",length=350)\n",
        "tokenizer.enable_truncation(max_length=350)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5wyOM7un7sv"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNOER8WXfppE"
      },
      "outputs": [],
      "source": [
        "\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class NLIDataset(Dataset):\n",
        "    def __init__(self, split='test'):\n",
        "\n",
        "      self.data = load_dataset('sentence-transformers/all-nli', 'pair-class', split=split)\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        label=item[\"label\"]\n",
        "        features=[[item['premise'], item['hypothesis']]]\n",
        "\n",
        "\n",
        "\n",
        "        self.features=tokenizer.encode_batch(features)\n",
        "        x = torch.tensor([encoding.ids for encoding in self.features], dtype=torch.long)\n",
        "        segment_id = torch.tensor([encoding.type_ids for encoding in self.features], dtype=torch.long)\n",
        "        mask = torch.tensor([encoding.attention_mask for encoding in self.features], dtype=torch.float)\n",
        "\n",
        "        return {\"input\":x,\"segment_id\":segment_id,\"mask\":mask,\"label\":torch.tensor(label, dtype=torch.long)}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = NLIDataset(split='test')\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "K9yEsF0iTftp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm30zBlwm8x6"
      },
      "source": [
        "# Model setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVWyecnvmmNb"
      },
      "outputs": [],
      "source": [
        "\n",
        "epochs=3\n",
        "batch_size=32\n",
        "lr=3e-4 #karpathy_constant\n",
        "device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "\n",
        "model,classifier=build_transformer(vocab_size,n_segments,embedd_dim,max_len,n_layers,attn_heads,dropout,d_ff,d_ff,True,2)\n",
        "model=model.to(device)\n",
        "classifier=classifier.to(device)\n",
        "\n",
        "model_optimizer=torch.optim.Adam(model.parameters(),lr=lr)\n",
        "cls_optimizer=torch.optim.Adam(classifier.parameters(),lr=lr)\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCKFGIhypAm3"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJxTT0W8bJgi"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import gc\n",
        "model.train()\n",
        "classifier.train()\n",
        "for epoch in range(3):\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    loop = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "    for batch in loop:\n",
        "        x = batch[\"input\"].squeeze(1).to(device)\n",
        "        segment_id = batch[\"segment_id\"].squeeze(1).to(device)\n",
        "        mask = batch[\"mask\"].squeeze(1).to(device)\n",
        "        labels = batch[\"label\"].to(device).unsqueeze(1)  # shape: [B, 1]\n",
        "        # print(f\"x {x.shape}, segment_id{segment_id.shape},mask {mask.shape} labels{labels.shape}\")\n",
        "\n",
        "        logits = model(x, segment_id, mask)  # shape: [B, 1]\n",
        "        logits=classifier(logits)\n",
        "\n",
        "        labels=labels.squeeze(1)\n",
        "        loss = loss_fn(logits, labels)\n",
        "\n",
        "        model_optimizer.zero_grad()\n",
        "        cls_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        model_optimizer.step()\n",
        "        cls_optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Metrics\n",
        "        preds = (torch.argmax(logits,dim=-1)).float()\n",
        "        correct += torch.sum((preds == labels).float())\n",
        "        total += labels.size(0)\n",
        "\n",
        "        loop.set_postfix(Loss=loss.item(), Accuracy=(preds==labels).float().mean().item())\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    avg_loss = train_loss / len(data_loader)\n",
        "    accuracy = correct / total\n",
        "    writer.add_scalar(\"train/loss\",avg_loss,epoch+1)\n",
        "    writer.add_scalar(\"train/accuracy\",accuracy,epoch+1)\n",
        "    print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "    global_step += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCM_P8XXgGqA"
      },
      "source": [
        "# MNRL Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Df5ogxGZgLEc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def mnr_loss(q_emb: torch.Tensor, p_emb: torch.Tensor, margin: float = 0.2) -> torch.Tensor:\n",
        "\n",
        "\n",
        "    q_norm = F.normalize(q_emb, p=2, dim=1)\n",
        "    p_norm = F.normalize(p_emb, p=2, dim=1)\n",
        "\n",
        "\n",
        "    sim_matrix = q_norm @ p_norm.t()  # cosine similarities\n",
        "\n",
        "    # 3. For each i:\n",
        "    pos_scores = sim_matrix.diag().unsqueeze(1)  # [N, 1]\n",
        "    neg_scores = sim_matrix  # [N, N] (including diagonals)\n",
        "\n",
        "\n",
        "    diff = pos_scores - neg_scores + margin\n",
        "\n",
        "    diff.fill_diagonal_(0.0)\n",
        "    loss_per_element = F.relu(diff)  # max(0, ...)\n",
        "\n",
        "    loss = loss_per_element.sum(dim=1).mean()\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sREvEin2voei"
      },
      "source": [
        "# NQ Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHAgVoItGk8U"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class NQDataset(Dataset):\n",
        "  def __init__(self,split=\"train\"):\n",
        "    self.ds = load_dataset(\"sentence-transformers/natural-questions\",split=split)\n",
        "  def __len__(self):return len(self.ds)\n",
        "  def __getitem__(self,idx):\n",
        "    item=self.ds[idx]\n",
        "    query=item[\"query\"]\n",
        "    answer=item[\"answer\"]\n",
        "    data=[query]\n",
        "    data=tokenizer.encode_batch(data)\n",
        "    tokens=torch.tensor([encoding.ids for encoding in data],dtype=torch.long)\n",
        "    segment_id=torch.tensor([encoding.type_ids for encoding in data],dtype=torch.long)\n",
        "    mask=torch.tensor([encoding.attention_mask for encoding in data],dtype=torch.long)\n",
        "\n",
        "    passage=tokenizer.encode_batch([answer])\n",
        "    passage_tokens=torch.tensor([encoding.ids for encoding in passage],dtype=torch.long)\n",
        "    passage_segment_id=torch.tensor([encoding.type_ids for encoding in passage],dtype=torch.long)\n",
        "    passage_mask=torch.tensor([encoding.attention_mask for encoding in passage],dtype=torch.long)\n",
        "\n",
        "    return {\"token\":tokens,\"segment_id\":segment_id,\"mask\":mask,\"passage_token\":passage_tokens,\"passage_segment_id\":passage_segment_id,\"passage_mask\":passage_mask}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgK6dSFHisZj"
      },
      "outputs": [],
      "source": [
        "nq_dataset=NQDataset()\n",
        "nq_data_loader=DataLoader(nq_dataset,batch_size=8,shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFvP-jrRjPx4"
      },
      "source": [
        "# MNRL Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LicBygh1fpFl"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import gc\n",
        "model.train()\n",
        "# classifier.train()\n",
        "for epoch in range(1):\n",
        "    train_loss = 0.0\n",
        "    loop = tqdm(nq_data_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "    for batch in loop:\n",
        "        query_token = batch[\"token\"].squeeze(1).to(device)\n",
        "        query_segment_id = batch[\"segment_id\"].squeeze(1).to(device)\n",
        "        query_mask = batch[\"mask\"].squeeze(1).to(device)\n",
        "\n",
        "        passage_token=batch[\"passage_token\"].squeeze(1).to(device)\n",
        "        passage_segment_id=batch[\"passage_segment_id\"].squeeze(1).to(device)\n",
        "        passage_mask=batch[\"passage_mask\"].squeeze(1).to(device)\n",
        "\n",
        "\n",
        "        q_emb=model(query_token,query_segment_id,query_mask)\n",
        "        p_emb=model(passage_token,passage_segment_id,passage_mask)\n",
        "\n",
        "        loss = mnr_loss(q_emb,p_emb)\n",
        "        model_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        model_optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loop.set_postfix(Loss=loss.item())\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = train_loss / len(marco_data_loader)\n",
        "    writer.add_scalar(\"mnr/train/loss\",avg_loss,epoch+1)\n",
        "    print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s2xhtpGv4P6"
      },
      "source": [
        "# Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1szUCrc9VOqd"
      },
      "outputs": [],
      "source": [
        "torch.save({\"model\":model.state_dict(),\"epoch\":2},\"/content/model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"/content/model.pt\")"
      ],
      "metadata": {
        "id": "2Rj7supQczOY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80a4d131-3604-4711-f008-93c69e8a487f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f51ffda1-9a3f-43b9-b8c5-a1444850d9f8\", \"model_trained_on_MARCO_1.pt\", 247708195)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}