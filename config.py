import torch
vocab_size=30000
special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]"]
n_segments=2
max_len=350
embedd_dim=768
n_layers=8
attn_heads=12
dropout=0.1
d_ff=1600
device = "cuda" if torch.cuda.is_available() else "cpu"
nli_batch_size=32
nq_batch_size=32
nli_epochs=10       
nq_epochs=10       
nli_lr=3e-4
nq_lr=3e-4
tokenizer_file_path="tokenizer-wiki.json"
model_file_path="models\phase_2_best (1).pt"
